{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0ed93b1-0bae-4659-84ab-52f44c63d46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0939b42-0d14-4557-9a6d-a41538c19383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy evaluation with Monte Carlo\n",
    "def eval_policy(policy, eval_env, eval_episodes=10):\n",
    "        avg_reward = 0.0\n",
    "        for _ in range(eval_episodes):\n",
    "            state, _ = eval_env.reset()\n",
    "            done = False\n",
    "            step = 0\n",
    "            while not done:\n",
    "                action = policy.select_action(np.array(state))\n",
    "                state, reward, terminated, _, _ = eval_env.step(action)\n",
    "                avg_reward += reward\n",
    "                step += 1\n",
    "                done = terminated\n",
    "        avg_reward /= eval_episodes\n",
    "\n",
    "        print(\"---------------------------------------\")\n",
    "        print(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
    "        print(\"---------------------------------------\")\n",
    "        return avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc059d4-467a-485e-bb61-7cbf89592e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay buffer\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, state_dim, action_dim, max_size=int(1e6)):\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "\n",
    "        self.state = np.zeros((max_size, state_dim))\n",
    "        self.action = np.zeros((max_size, action_dim))\n",
    "        self.next_state = np.zeros((max_size, state_dim))\n",
    "        self.reward = np.zeros((max_size, 1))\n",
    "        self.not_done = np.zeros((max_size, 1))\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "    def add(self, state, action, next_state, reward, done):\n",
    "        self.state[self.ptr] = state\n",
    "        self.action[self.ptr] = action\n",
    "        self.next_state[self.ptr] = next_state\n",
    "        self.reward[self.ptr] = reward\n",
    "        self.not_done[self.ptr] = 1. - done\n",
    "\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, self.size, size=batch_size)\n",
    "\n",
    "        return (\n",
    "            torch.FloatTensor(self.state[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.action[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.next_state[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.reward[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.not_done[ind]).to(self.device)\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
