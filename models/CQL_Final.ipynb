{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9LZWV8StiD_"
   },
   "source": [
    "The following code is our implementation of Conservative Q-Learning (CQL) for offline RL over the encoded MathDial dataset (we discuss the state-action space more in our paper).\n",
    "\n",
    "The implementation of CQL (particularly the replay buffer design and loss fuction) draws reference from:\n",
    "- Aviral Kumar et al. \"Conservative Q-Learning for Offline Reinforcement Learning\" (https://arxiv.org/abs/2006.04779)\n",
    "- Reference implementation: https://github.com/aviralkumar2907/CQL (official implementation by paper authors)\n",
    "- Reference implementation: https://github.com/BY571/CQL (for discrete action space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FcaclPW5tfLF"
   },
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tF7LZOUGwYPt"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper to create a train test split since sklearn train_test_split won't work for our dataset's object type. We split the dataframe at episode boundaries using 'done' flags.\n",
    "\"\"\"\n",
    "def split_dataframe_by_episodes(df, test_size=0.2, val_size=0.1, random_state=42):\n",
    "    episode_boundaries = []\n",
    "    start = 0\n",
    "    for i, row in df.iterrows():\n",
    "        if row['done'] == 1:\n",
    "            episode_boundaries.append((start, i + 1))\n",
    "            start = i + 1\n",
    "    if start < len(df):\n",
    "        episode_boundaries.append((start, len(df)))\n",
    "\n",
    "    train_val_eps, test_eps = train_test_split(\n",
    "        episode_boundaries,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    rel_val_size = val_size / (1 - test_size)\n",
    "    train_eps, val_eps = train_test_split(\n",
    "        train_val_eps,\n",
    "        test_size=rel_val_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    def collect(indices):\n",
    "        idx = []\n",
    "        for s, e in indices:\n",
    "            idx.extend(range(s, e))\n",
    "        return df.iloc[idx].reset_index(drop=True)\n",
    "\n",
    "    train_df = collect(train_eps)\n",
    "    val_df   = collect(val_eps)\n",
    "    test_df  = collect(test_eps)\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "def split_dataframe_by_episodes_no_val(df, test_size=0.2, random_state=42):\n",
    "    episode_boundaries = []\n",
    "    current_episode_start = 0\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        if row['done'] == 1:  # end of an episode\n",
    "            episode_boundaries.append((current_episode_start, i + 1))\n",
    "            current_episode_start = i + 1\n",
    "\n",
    "    if current_episode_start < len(df):\n",
    "        episode_boundaries.append((current_episode_start, len(df)))\n",
    "\n",
    "    # split episode boundaries\n",
    "    train_episodes, test_episodes = train_test_split(\n",
    "        episode_boundaries,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "\n",
    "    for start, end in train_episodes:\n",
    "        train_indices.extend(range(start, end))\n",
    "\n",
    "    for start, end in test_episodes:\n",
    "        test_indices.extend(range(start, end))\n",
    "\n",
    "    train_df = df.iloc[train_indices].reset_index(drop=True)\n",
    "    test_df = df.iloc[test_indices].reset_index(drop=True)\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Npafb0XDwZLG"
   },
   "outputs": [],
   "source": [
    "# Helper class to convert numpy arrays to Pytorch tensors for custom dataset\n",
    "class TutorDataset(Dataset):\n",
    "    def __init__(self, states, actions):\n",
    "        self.states = torch.tensor(states, dtype=torch.float32)\n",
    "        self.actions = torch.tensor(actions, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.actions)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.states[i], self.actions[i]\n",
    "\n",
    "# Implements standard BC using FFNN w/ 2 hidden layers, 256 units per layer, ReLU activation, no dropout\n",
    "class BCModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def prepare_data_for_bc(df, encoder=None):\n",
    "    state_features = ['misconception_type','convo_turn','previous_action_id',\n",
    "                   'listen_to_feedback','problem_progress','progress_delta',\n",
    "                   'correct_solution','next_action_hint_strength']\n",
    "\n",
    "    categorical_features = ['misconception_type','previous_action_id',\n",
    "                 'listen_to_feedback','correct_solution','next_action_hint_strength']\n",
    "    numerical_features = [f for f in state_features if f not in categorical_features]\n",
    "\n",
    "    action_column = 'next_action_id'\n",
    "    df = df.dropna(subset=[action_column])\n",
    "    df[action_column] = df[action_column].astype(int)\n",
    "    for feature in categorical_features:\n",
    "        df[feature] = df[feature].fillna(-1)\n",
    "    for feature in numerical_features:\n",
    "        df[feature] = df[feature].fillna(df[feature].mean() if not df[feature].isna().all() else 0)\n",
    "\n",
    "    if encoder is None:\n",
    "        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        encoded_features = encoder.fit_transform(df[categorical_features])\n",
    "    else:\n",
    "        encoded_features = encoder.transform(df[categorical_features])\n",
    "\n",
    "    numerical_data = df[numerical_features].values\n",
    "    states = np.hstack((numerical_data, encoded_features))\n",
    "    actions = df[action_column].values\n",
    "    unique_actions = np.unique(actions)\n",
    "    action_map = {old_id: new_id for new_id, old_id in enumerate(sorted(unique_actions))}\n",
    "    remapped_actions = np.array([action_map[a] for a in actions])\n",
    "    train_states, val_states, train_actions, val_actions = train_test_split(\n",
    "        states, remapped_actions, test_size=0.2, random_state=42\n",
    "    )\n",
    "    return train_states, val_states, train_actions, val_actions, len(unique_actions)\n",
    "\n",
    "def train_bc_model(train_states, val_states, train_actions, val_actions, num_actions, epochs=50, batch_size=128, lr=1e-3):\n",
    "    model = BCModel(input_dim=train_states.shape[1], output_dim=num_actions)\n",
    "\n",
    "    train_ds = TutorDataset(train_states, train_actions)\n",
    "    val_ds = TutorDataset(val_states, val_actions)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_acc = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        for S, A in train_loader:\n",
    "            logits = model(S)\n",
    "            loss = criterion(logits, A)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            train_total += A.size(0)\n",
    "            train_correct += (predicted == A).sum().item()\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_total = 0\n",
    "        val_correct = 0\n",
    "        with torch.no_grad():\n",
    "            for S, A in val_loader:\n",
    "                logits = model(S)\n",
    "                loss = criterion(logits, A)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(logits.data, 1)\n",
    "                val_total += A.size(0)\n",
    "                val_correct += (predicted == A).sum().item()\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_bc_model.pt')\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "    print(f'Best val accuracy: {best_val_acc:.2f}%')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "teNpLZxHwaYX"
   },
   "outputs": [],
   "source": [
    "# replay buffer: stores sequences of state-action-reward transitions (s,a,r,s',done)\n",
    "class TransitionBuffer:\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.next_states = []\n",
    "        self.dones = []\n",
    "\n",
    "    def push(self, s, a, r, s2, done):\n",
    "        self.states.append(s)\n",
    "        self.actions.append(a)\n",
    "        self.rewards.append(r)\n",
    "        self.next_states.append(s2)\n",
    "        self.dones.append(done)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.actions)\n",
    "\n",
    "def build_transition_buffer(df, reward_fn, meta_map, orig_to_idx, encoder=None, categorical_features=None, episode_column=None):\n",
    "    flat_idx_to_meta = {\n",
    "        idx: {'category': cat, 'strategy': strat, 'level': lvl}\n",
    "        for cat, strategies in meta_map.items()\n",
    "        for strat, levels in strategies.items()\n",
    "        for lvl, idx in levels.items()\n",
    "    }\n",
    "\n",
    "    state_features = [\n",
    "        'misconception_type','convo_turn','previous_action_id',\n",
    "        'listen_to_feedback','problem_progress','progress_delta',\n",
    "        'correct_solution','next_action_hint_strength'\n",
    "    ]\n",
    "    if categorical_features is None:\n",
    "        categorical_features = [\n",
    "            'misconception_type','previous_action_id',\n",
    "            'listen_to_feedback','correct_solution','next_action_hint_strength'\n",
    "        ]\n",
    "    num_feats = [f for f in state_features if f not in categorical_features]\n",
    "\n",
    "    if encoder is None:\n",
    "        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        encoder.fit(df[categorical_features])\n",
    "\n",
    "    cat_array = encoder.transform(df[categorical_features].values)\n",
    "\n",
    "    if episode_column:\n",
    "        episodes = df.sort_values([episode_column, 'convo_turn']).groupby(episode_column)\n",
    "    else:\n",
    "        df = df.sort_values('convo_turn')\n",
    "        df['_tmp_ep'] = 1\n",
    "        episodes = df.groupby('_tmp_ep')\n",
    "\n",
    "    buffer = TransitionBuffer()\n",
    "\n",
    "    # build transitions per episode\n",
    "    for _, ep_df in episodes:\n",
    "        prev_state = prev_idx = prev_meta = None\n",
    "\n",
    "        for idx_row in ep_df.index:\n",
    "            row = df.loc[idx_row]\n",
    "            orig_id = row.get('next_action_id')\n",
    "            if pd.isna(orig_id) or int(orig_id) not in orig_to_idx:\n",
    "                continue\n",
    "            action_idx = orig_to_idx[int(orig_id)]\n",
    "\n",
    "            num = row[num_feats].to_numpy()\n",
    "            cat = cat_array[idx_row]\n",
    "            state = np.hstack((num, cat))\n",
    "            meta = flat_idx_to_meta.get(action_idx)\n",
    "            done_flag = bool(row.get('done', False))\n",
    "\n",
    "            if prev_state is not None:\n",
    "                r = reward_fn(prev_state, prev_idx, state, prev_meta)\n",
    "                buffer.push(prev_state, prev_idx, r, state, done_flag)\n",
    "\n",
    "            prev_state, prev_idx, prev_meta = state, action_idx, meta\n",
    "\n",
    "            if done_flag:\n",
    "                term_r = new_terminal(state)\n",
    "                buffer.push(state, action_idx, term_r, state, True)\n",
    "                prev_state = prev_idx = prev_meta = None\n",
    "\n",
    "    buffer.states = np.array(buffer.states)\n",
    "    buffer.actions = np.array(buffer.actions, dtype=int)\n",
    "    buffer.rewards = np.array(buffer.rewards, dtype=float)\n",
    "    buffer.next_states = np.array(buffer.next_states)\n",
    "    buffer.dones = np.array(buffer.dones, dtype=bool)\n",
    "\n",
    "    return buffer, orig_to_idx\n",
    "\n",
    "\n",
    "def new_terminal(state):\n",
    "    MAX_PROGRESS = 50.0 # to make sure the raw progress reward isn't greater than if the solution was just correct\n",
    "    raw_progress = state[4]  # problem_progress in state\n",
    "    normalized_progress = min(raw_progress, MAX_PROGRESS) / MAX_PROGRESS\n",
    "    # want a higher reward if correct solution was achieved\n",
    "    return 5.0 if state[6] > 0 else 2.0 * normalized_progress\n",
    "\n",
    "\"\"\"\n",
    "Our custom designed reward function that follows the principal of: scaffolding then telling. More about our reward function design is in the paper.\n",
    "\"\"\"\n",
    "def hybrid_reward(state, action_id, next_state=None, action_meta=None):\n",
    "    progress_delta = state[5]\n",
    "    turn = state[1]\n",
    "    listen_to_feedback = state[3]\n",
    "\n",
    "    # progress reward and step penalty\n",
    "    progress_reward = 5.0 * progress_delta\n",
    "    step_penalty = -0.1\n",
    "\n",
    "    # how far into the conversation the current transition is\n",
    "    turn_progress = min(1.0, turn / 8.0)\n",
    "\n",
    "    # will be used for the pedagoical/action-specific reward bonus\n",
    "    strategy_bonus = 0.0\n",
    "    if action_meta is not None:\n",
    "        cat = action_meta['category']\n",
    "\n",
    "        # scaffolding actions (Focus, Probing)\n",
    "        if cat in ['Focus', 'Probing']:\n",
    "            strategy_bonus = 0.2 * (1.0 - turn_progress) # decreasing bonus for scaffolding actions as conversation progresses\n",
    "\n",
    "        # telling actions\n",
    "        elif cat == 'Telling':\n",
    "            severity = 1.0\n",
    "            if 'strategy' in action_meta:\n",
    "                if action_meta['strategy'] == 'Full Reveal (Answer)':\n",
    "                    severity = 1.5 # penalize more for strong hints\n",
    "                elif action_meta['strategy'] == 'Conceptual Hint':\n",
    "                    severity = 0.6\n",
    "\n",
    "            # if early in convo: penalty for telling\n",
    "            early_penalty = -0.3 * (1.0 - turn_progress) * severity\n",
    "            # if later in convo: bonus for telling so long as effective (the student makes good progress)\n",
    "            late_bonus = 0.0\n",
    "            if progress_delta > 0:\n",
    "                late_bonus = 0.2 * turn_progress\n",
    "\n",
    "            strategy_bonus = early_penalty + late_bonus\n",
    "\n",
    "    return progress_reward + step_penalty + 2.0*strategy_bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WloDAKqAwbiZ"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper function that is essentially asking: if we started from this random state in the buffer and followed the agent's policy, what rewards would the agent have earned if the environment happened to transition exactly as recorded in the buffer?\n",
    "\n",
    "Terminates if the conversation is done, or by max steps (should be 80), or if the buffer ends.\n",
    "\"\"\"\n",
    "def evaluate_policy(agent, buffer, reward_fn, num_episodes=20, max_steps=80, gamma=0.98, action_meta_map=None):\n",
    "    if hasattr(agent, 'q_net') and hasattr(agent.q_net, 'eval'):\n",
    "        agent.q_net.eval()\n",
    "    elif hasattr(agent, 'model') and hasattr(agent.model, 'eval'):\n",
    "        agent.model.eval()\n",
    "\n",
    "    flat_idx_to_meta = {}\n",
    "    if action_meta_map:\n",
    "        for cat, strategies in action_meta_map.items():\n",
    "            for strat, levels in strategies.items():\n",
    "                for lvl, idx in levels.items():\n",
    "                    flat_idx_to_meta[idx] = {\n",
    "                        'category': cat,\n",
    "                        'strategy': strat,\n",
    "                        'level': lvl\n",
    "                    }\n",
    "\n",
    "    total_returns = []\n",
    "    episode_lengths = []\n",
    "    termination_reasons = {\"max_steps\": 0, \"done\": 0, \"buffer_end\": 0}\n",
    "\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        # sample a random starting state\n",
    "        start_idx = np.random.randint(0, len(buffer) - max_steps)\n",
    "        if isinstance(buffer.states[start_idx], torch.Tensor):\n",
    "            state = buffer.states[start_idx].cpu().numpy()\n",
    "        else:\n",
    "            state = buffer.states[start_idx].copy()\n",
    "\n",
    "        episode_return = 0\n",
    "        episode_length = 0\n",
    "        discount = 1.0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # select action using the policy\n",
    "            action_idx = agent.select_action(state)\n",
    "\n",
    "            \"\"\"\n",
    "            IMPORTANT: the chosen action is ignored (since we don't have a simulator to execute chosen action to transition to next state), so we just move sequentially through buffer.\n",
    "            Rewards are calculated by the action the agent chose.\n",
    "            \"\"\"\n",
    "            # get next state by sampling from buffer\n",
    "            next_idx = min(start_idx + step + 1, len(buffer) - 1)\n",
    "\n",
    "            if isinstance(buffer.states[next_idx], torch.Tensor):\n",
    "                next_state = buffer.states[next_idx].cpu().numpy()\n",
    "            else:\n",
    "                next_state = buffer.states[next_idx].copy()\n",
    "\n",
    "            # check for done flag\n",
    "            done = False\n",
    "            if hasattr(buffer, 'dones') and len(buffer.dones) > next_idx:\n",
    "                done = buffer.dones[next_idx]\n",
    "\n",
    "            action_meta = None\n",
    "            if action_meta_map:\n",
    "                action_meta = flat_idx_to_meta.get(action_idx)\n",
    "\n",
    "            # compute reward\n",
    "            reward = reward_fn(state, action_idx, next_state, action_meta)\n",
    "            # update return\n",
    "            episode_return += discount * reward\n",
    "            discount *= gamma\n",
    "            episode_length += 1\n",
    "            # update state\n",
    "            state = next_state\n",
    "\n",
    "            # check for termination\n",
    "            if done:\n",
    "                termination_reasons[\"done\"] += 1\n",
    "                break\n",
    "\n",
    "            # reached max number of steps (some episodes were too long, like > 180 turns)\n",
    "            if step >= max_steps - 1:\n",
    "                termination_reasons[\"max_steps\"] += 1\n",
    "                break\n",
    "\n",
    "            # reached the end of the buffer\n",
    "            if next_idx >= len(buffer) - 1:\n",
    "                termination_reasons[\"buffer_end\"] += 1\n",
    "                # print(f\"  Episode terminated due to buffer end at step {step+1}\")\n",
    "                break\n",
    "\n",
    "        total_returns.append(episode_return)\n",
    "        episode_lengths.append(episode_length)\n",
    "\n",
    "    mean_return = np.mean(total_returns)\n",
    "    std_return = np.std(total_returns)\n",
    "\n",
    "    print(f\"\\nEvaluation over {num_episodes} episodes:\")\n",
    "    print(f\"  Mean return: {mean_return:.4f} ± {std_return:.4f}\")\n",
    "    print(f\"  Mean episode length: {np.mean(episode_lengths):.2f}\")\n",
    "    print(f\"  Termination reasons: {termination_reasons}\")\n",
    "\n",
    "    return mean_return, total_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SIIHh_IPwcx2"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a class to wrap the BC model to make it more compatible with the evaluation framework when I do the BC-init (warm start).\n",
    "1. Takes a trained BC model and an action mapping dictionary\n",
    "2. Has a select_action method that takes a state and returns an action index\n",
    "\"\"\"\n",
    "class BCPolicyWrapper:\n",
    "    def __init__(self, model, action_map, device=\"cpu\"):\n",
    "        self.model = model.to(device)\n",
    "        self.q_net = self.model  # alias for compatibility\n",
    "        self.inv_map = {v: k for k, v in action_map.items()}\n",
    "        self.device = device\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        else:\n",
    "            if state.dim() == 1:\n",
    "                state = state.unsqueeze(0)\n",
    "            state = state.to(self.device)\n",
    "\n",
    "        # return the action with the highest logit value (highest prob)\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(state)\n",
    "            action = logits.argmax(dim=1).item()\n",
    "        return action\n",
    "\n",
    "def main():\n",
    "    csv_file_path = \"data.csv\"\n",
    "\n",
    "    # predefined action mappings\n",
    "    action_map = {0: 0, 1: 1, 2: 2, 3: 3, 5: 4, 6: 5, 7: 6, 8: 7, 11: 8, 12: 9,\n",
    "              13: 10, 16: 11, 17: 12, 18: 13, 20: 14, 21: 15, 22: 16, 23: 17,\n",
    "              26: 18, 27: 19, 28: 20, 31: 21, 32: 22, 36: 23, 37: 24, 38: 25,\n",
    "              41: 26, 42: 27, 43: 28, 45: 29, 46: 30, 47: 31, 48: 32, 54: 33,\n",
    "              55: 34, 56: 35, 57: 36, 58: 37, 59: 38, 60: 39, 65: 40, 66: 41,\n",
    "              67: 42, 70: 43, 71: 44, 72: 45, 73: 46, 75: 47, 76: 48, 77: 49}\n",
    "\n",
    "    action_meta_map = {\n",
    "        \"Focus\": {\n",
    "            \"Seek Next Step\": {1: 0, 2: 1, 3: 2},\n",
    "            \"Confirm Calculation\": {1: 5, 2: 6, 3: 7, 4: 8},\n",
    "            \"Re-direct to Sub-Problem\": {2: 11, 3: 12, 4: 13},\n",
    "            \"Highlight Missing Info\": {2: 16, 3: 17, 4: 18}\n",
    "        },\n",
    "        \"Probing\": {\n",
    "            \"Ask for Explanation\": {1: 20, 2: 21, 3: 22, 4: 23},\n",
    "            \"Seek Self-Correction\": {2: 26, 3: 27, 4: 28},\n",
    "            \"Hypothetical Variation\": {2: 31, 3: 32},\n",
    "            \"Check Understanding/Concept\": {2: 36, 3: 37, 4: 38},\n",
    "            \"Encourage Comparison\": {2: 41, 3: 42, 4: 43}\n",
    "        },\n",
    "        \"Telling\": {\n",
    "            \"Partial Reveal (Strategy)\": {1: 45, 2: 46, 3: 47, 4: 48},\n",
    "            \"Full Reveal (Answer)\": {1: 54, 2: 55, 3: 56, 4: 57, 5: 58, 6: 59},\n",
    "            \"Corrective Explanation\": {1: 60}\n",
    "        },\n",
    "        \"Generic\": {\n",
    "            \"Acknowledgment/Praise\": {1: 65, 2: 66, 3: 67},\n",
    "            \"Summarize Progress\": {1: 70, 2: 71, 3: 72, 4: 73},\n",
    "            \"General Inquiry/Filler\": {1: 75, 2: 76, 3: 77}\n",
    "        }\n",
    "    }\n",
    "\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    # build transition buffer\n",
    "    buffer, action_mapping = build_transition_buffer(\n",
    "        df=df,\n",
    "        reward_fn=hybrid_reward,\n",
    "        meta_map=action_meta_map,\n",
    "        orig_to_idx=action_map,\n",
    "        episode_column=None\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYzsCRRtwd_J"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initialize a CQL agent using weights from a pre-trained BC model\n",
    "Motivation: to give CQL a good starting point, since BC has already learning meaningful state representations (warm start)\n",
    "\"\"\"\n",
    "def convert_bc_to_cql_model(bc_model, action_dim):\n",
    "    input_dim = bc_model.net[0].in_features\n",
    "    cql_agent = CQLAgent(state_dim=input_dim, action_dim=action_dim)\n",
    "    cql_agent.q_net.net[0].weight.data.copy_(bc_model.net[0].weight.data)\n",
    "    cql_agent.q_net.net[0].bias.data.copy_(bc_model.net[0].bias.data)\n",
    "    cql_agent.q_net.net[2].weight.data.copy_(bc_model.net[2].weight.data)\n",
    "    cql_agent.q_net.net[2].bias.data.copy_(bc_model.net[2].bias.data)\n",
    "    cql_agent.target_q_net.load_state_dict(cql_agent.q_net.state_dict())\n",
    "    return cql_agent\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\"\"\"\n",
    "Why CQL?\n",
    "It's offline and good to ensure a conservatively learned policy - don't overestimate out of distribution action rewards.\n",
    "\n",
    "Vanilla QL overestimates values for unseen (s,a) pairs, and so CQL adds a regularization term that penalizes Q values for actions not seen in the dataset to make the policy more conservative. This is important in educaitonal settings that can affect student learning outcomes. Also, we don't have a good simulator to interact with the environment.\n",
    "\"\"\"\n",
    "class CQLAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        hidden_dim=256,\n",
    "        lr=3e-4,\n",
    "        gamma=0.98,\n",
    "        tau=0.005,\n",
    "        cql_alpha=1.0,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    ):\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.cql_alpha = cql_alpha\n",
    "        self.device = device\n",
    "\n",
    "        # need 2 Q-networks, target network used for stable learning\n",
    "        self.q_net = QNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.target_q_net = QNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.target_q_net.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "        # freeze target network (prevent moving target problem)\n",
    "        for param in self.target_q_net.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=lr)\n",
    "\n",
    "        self.training_stats = {\n",
    "            'q_loss': [],\n",
    "            'cql_loss': [],\n",
    "            'total_loss': [],\n",
    "            'avg_q_values': []\n",
    "        }\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # select the best action according to the Q-network\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            q_values = self.q_net(state)\n",
    "            action = q_values.argmax(dim=1).item()\n",
    "        return action\n",
    "\n",
    "    # MOST IMPORTANT!\n",
    "    \"\"\"\n",
    "    NOTE: The CQL loss implementation is adapted from:\n",
    "    https://github.com/BY571/CQL/blob/main/CQL-DQN/agent.py\n",
    "    with some modifications for our specific use case.\n",
    "    \"\"\"\n",
    "    def update(self, batch):\n",
    "        # update the Q network using CQL loss\n",
    "        states, actions, rewards, next_states, dones = batch\n",
    "        states = states.to(self.device)\n",
    "        actions = actions.to(self.device)\n",
    "        rewards = rewards.to(self.device)\n",
    "        next_states = next_states.to(self.device)\n",
    "        dones = dones.to(self.device)\n",
    "\n",
    "        batch_size = states.shape[0]\n",
    "\n",
    "        # compute Q values and target Q values\n",
    "        q_values = self.q_net(states)\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_q_net(next_states)\n",
    "            next_actions = next_q_values.argmax(dim=1)\n",
    "            next_q = next_q_values.gather(1, next_actions.unsqueeze(1)).squeeze()\n",
    "            target_q = rewards + (1 - dones) * self.gamma * next_q\n",
    "\n",
    "        # TD error for the sampled actions (like vanilla Q learning)\n",
    "        # Standard Bellman error\n",
    "        q_values_sampled = q_values.gather(1, actions.unsqueeze(1)).squeeze() # selects the Q values corresponding to the specific action indices that were chosen in the offline dataset (the action demonstrated by the expert at that state)\n",
    "        td_loss = F.mse_loss(q_values_sampled, target_q)\n",
    "\n",
    "        # CQL loss: Minimize Q values for actions not in the dataset\n",
    "        logsumexp_q = torch.logsumexp(q_values, dim=1)\n",
    "        \"\"\"\n",
    "        logSumExp(x₁, x₂, ..., xₙ) = log(exp(x₁) + exp(x₂) + ... + exp(xₙ))\n",
    "        A smooth approximation of the maximum function across the action dimension. When one value is significantly larger than the others, it dominates the sum inside the log (and is differentiable)\n",
    "        \"\"\"\n",
    "        # logsumexp_q is the 'soft' maximum Q value across all actions for each state\n",
    "        # q_values_sampled is the Q value for the actions in the dataset (actions demonstrated by the expert)\n",
    "        cql_loss = (logsumexp_q - q_values_sampled).mean()\n",
    "        \"\"\"\n",
    "        When the difference between the max  over all action values for each state and the q val of demonstrated actions is LARGE, it means the model is overestimating Q values for actions not in the dataset.\n",
    "        Th CQL objective is to MINIMIZE this difference by penalizing the model for overestimation.\n",
    "\n",
    "        This pushes Q values down for all non-dataset actions while preserving the values for actions the expert tutors actually took.\n",
    "        \"\"\"\n",
    "        # total loss\n",
    "        loss = td_loss + self.cql_alpha * cql_loss # add cql regularization term\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # update target network, periodic updates (low tau, less frequent updates) for stable learning\n",
    "        for param, target_param in zip(self.q_net.parameters(), self.target_q_net.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "        self.training_stats['q_loss'].append(td_loss.item())\n",
    "        self.training_stats['cql_loss'].append(cql_loss.item())\n",
    "        self.training_stats['total_loss'].append(loss.item())\n",
    "        self.training_stats['avg_q_values'].append(q_values.mean().item())\n",
    "\n",
    "        return td_loss.item(), cql_loss.item(), loss.item()\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save({\n",
    "            'q_net': self.q_net.state_dict(),\n",
    "            'target_q_net': self.target_q_net.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'training_stats': self.training_stats\n",
    "        }, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.q_net.load_state_dict(checkpoint['q_net'])\n",
    "        self.target_q_net.load_state_dict(checkpoint['target_q_net'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.training_stats = checkpoint['training_stats']\n",
    "\n",
    "    def plot_training_curves(self):\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "        # Plot Q loss\n",
    "        axs[0, 0].plot(self.training_stats['q_loss'])\n",
    "        axs[0, 0].set_title('TD Loss')\n",
    "        axs[0, 0].set_xlabel('Updates')\n",
    "        axs[0, 0].set_ylabel('Loss')\n",
    "\n",
    "        # Plot CQL loss\n",
    "        axs[0, 1].plot(self.training_stats['cql_loss'])\n",
    "        axs[0, 1].set_title('CQL Loss')\n",
    "        axs[0, 1].set_xlabel('Updates')\n",
    "        axs[0, 1].set_ylabel('Loss')\n",
    "\n",
    "        # Plot total loss\n",
    "        axs[1, 0].plot(self.training_stats['total_loss'])\n",
    "        axs[1, 0].set_title('Total Loss')\n",
    "        axs[1, 0].set_xlabel('Updates')\n",
    "        axs[1, 0].set_ylabel('Loss')\n",
    "\n",
    "        # Plot average Q values\n",
    "        axs[1, 1].plot(self.training_stats['avg_q_values'])\n",
    "        axs[1, 1].set_title('Average Q Values')\n",
    "        axs[1, 1].set_xlabel('Updates')\n",
    "        axs[1, 1].set_ylabel('Q Value')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('cql_training_curves.png')\n",
    "        plt.close()\n",
    "\n",
    "def prepare_buffer_for_training(buffer, batch_size=128, action_map=None):\n",
    "    states = torch.FloatTensor(np.array(buffer.states))\n",
    "\n",
    "    # map the original action IDs -> model indices\n",
    "    if action_map:\n",
    "        mapped_actions = [action_map.get(a, 0) for a in buffer.actions]\n",
    "        actions = torch.LongTensor(mapped_actions)\n",
    "    else:\n",
    "        actions = torch.LongTensor(buffer.actions)\n",
    "\n",
    "    rewards = torch.FloatTensor(np.array(buffer.rewards))\n",
    "    next_states = torch.FloatTensor(np.array(buffer.next_states))\n",
    "    dones = torch.FloatTensor(np.array(buffer.dones))\n",
    "\n",
    "    dataset = TensorDataset(states, actions, rewards, next_states, dones)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Train a CQL agent on the given transition buffer\n",
    "1. Prepare data from the transition buffer\n",
    "2. Initialize the CQL agent\n",
    "3. Train over multiple epochs\n",
    "4. Periodically evaluate performance\n",
    "4. Save the best model\n",
    "\"\"\"\n",
    "def train_cql(buffer, state_dim, action_dim, num_epochs=100, batch_size=128,\n",
    "              hidden_dim=256, lr=3e-4, gamma=0.98, tau=0.005, cql_alpha=1.0, eval_buffer=None,\n",
    "              eval_interval=5, model_save_path='cql_agent.pt', action_map=None, action_meta_map=None):\n",
    "    dataloader = prepare_buffer_for_training(buffer, batch_size, action_map)\n",
    "\n",
    "    agent = CQLAgent(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        lr=lr,\n",
    "        gamma=gamma,\n",
    "        tau=tau,\n",
    "        cql_alpha=cql_alpha\n",
    "    )\n",
    "\n",
    "    print(f\"Training CQL agent with {len(buffer)} transitions\")\n",
    "    print(f\"State dim: {state_dim}, Action dim: {action_dim}\")\n",
    "    print(f\"CQL alpha: {cql_alpha}, Learning rate: {lr}\")\n",
    "\n",
    "    best_eval_return = -float('inf')\n",
    "    buf = eval_buffer if eval_buffer is not None else buffer\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_td_loss = 0\n",
    "        epoch_cql_loss = 0\n",
    "        epoch_total_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        agent.q_net.train()\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            td_loss, cql_loss, total_loss = agent.update(batch)\n",
    "            epoch_td_loss += td_loss\n",
    "            epoch_cql_loss += cql_loss\n",
    "            epoch_total_loss += total_loss\n",
    "            num_batches += 1\n",
    "\n",
    "            progress_bar.set_postfix({\n",
    "                'td_loss': td_loss,\n",
    "                'cql_loss': cql_loss,\n",
    "                'total_loss': total_loss\n",
    "            })\n",
    "\n",
    "        # calculate average losses\n",
    "        epoch_td_loss /= num_batches\n",
    "        epoch_cql_loss /= num_batches\n",
    "        epoch_total_loss /= num_batches\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs} - TD Loss: {epoch_td_loss:.4f}, \"\n",
    "              f\"CQL Loss: {epoch_cql_loss:.4f}, Total Loss: {epoch_total_loss:.4f}\")\n",
    "\n",
    "        # evaluation\n",
    "        if (epoch + 1) % eval_interval == 0:\n",
    "            mean_return, _ = evaluate_policy(\n",
    "                agent=agent,\n",
    "                buffer=buf,\n",
    "                reward_fn=hybrid_reward,\n",
    "                num_episodes=20,\n",
    "                max_steps=80,\n",
    "                action_meta_map=action_meta_map\n",
    "            )\n",
    "            print(f\"Evaluation - Mean Return: {mean_return:.4f}\")\n",
    "\n",
    "            # save best model\n",
    "            if mean_return > best_eval_return:\n",
    "                best_eval_return = mean_return\n",
    "                agent.save(model_save_path)\n",
    "                print(f\"New best model saved with return: {best_eval_return:.4f}\")\n",
    "\n",
    "    agent.plot_training_curves()\n",
    "    agent.load(model_save_path)\n",
    "    final_mean_return, final_returns = evaluate_policy(\n",
    "        agent=agent,\n",
    "        buffer=buf,\n",
    "        reward_fn=hybrid_reward,\n",
    "        num_episodes=50,\n",
    "        max_steps=60\n",
    "    )\n",
    "    print(f\"\\nFINAL: Mean Return: {final_mean_return:.4f}\")\n",
    "\n",
    "    return agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ykSdhXexwfbS"
   },
   "outputs": [],
   "source": [
    "def compute_confidence_interval(data, confidence=0.95):\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    sem = stats.sem(data)\n",
    "    h = sem * stats.t.ppf((1 + confidence) / 2, n-1)\n",
    "    return mean, mean - h, mean + h\n",
    "\n",
    "class RandomPolicy:\n",
    "    def __init__(self, action_dim):\n",
    "        self.action_dim = action_dim\n",
    "    def select_action(self, state):\n",
    "        return np.random.randint(self.action_dim)\n",
    "\n",
    "def evaluate_all_models_on_same_states(\n",
    "    agents, labels, buffer, reward_fn,\n",
    "    num_episodes=50, max_steps=80, action_meta_map=None\n",
    "):\n",
    "\n",
    "    flat_idx_to_meta = {\n",
    "        idx: {'category': cat, 'strategy': strat, 'level': lvl}\n",
    "        for cat, strategies in action_meta_map.items()\n",
    "        for strat, levels in strategies.items()\n",
    "        for lvl, idx in levels.items()\n",
    "    }\n",
    "\n",
    "    # sample once\n",
    "    start_indices = np.random.randint(0, len(buffer) - max_steps, size=num_episodes)\n",
    "\n",
    "    # prepare storage\n",
    "    all_returns = {lbl: np.zeros(num_episodes) for lbl in labels}\n",
    "\n",
    "    for ep_idx, start_idx in enumerate(start_indices):\n",
    "        for agent, lbl in zip(agents, labels):\n",
    "            state = buffer.states[start_idx].copy()\n",
    "            episode_return = 0.0\n",
    "            discount = 1.0\n",
    "\n",
    "            for step in range(max_steps):\n",
    "                action_idx = agent.select_action(state)\n",
    "                next_idx = min(start_idx + step + 1, len(buffer) - 1)\n",
    "                next_state = buffer.states[next_idx].copy()\n",
    "                done = bool(buffer.dones[next_idx]) if hasattr(buffer, 'dones') else False\n",
    "                action_meta = flat_idx_to_meta.get(action_idx)\n",
    "                r = reward_fn(state, action_idx, next_state, action_meta)\n",
    "\n",
    "                episode_return += discount * r\n",
    "                discount *= 0.98\n",
    "                state = next_state\n",
    "                if done or next_idx >= len(buffer)-1:\n",
    "                    break\n",
    "\n",
    "            all_returns[lbl][ep_idx] = episode_return\n",
    "\n",
    "    return all_returns\n",
    "\n",
    "def collect_seed_stats(seed, agents, labels, buffer, reward_fn, action_meta_map, num_episodes=100, max_steps=60):\n",
    "    np.random.seed(seed)\n",
    "    results = evaluate_all_models_on_same_states(\n",
    "        agents,\n",
    "        labels,\n",
    "        buffer=buffer,\n",
    "        reward_fn=reward_fn,\n",
    "        num_episodes=num_episodes,\n",
    "        max_steps=max_steps,\n",
    "        action_meta_map=action_meta_map\n",
    "    )\n",
    "    seed_dict = {}\n",
    "    # compute mean and 95% CI for each method\n",
    "    for lbl in labels:\n",
    "        arr = results[lbl]\n",
    "        mean, lo, hi = compute_confidence_interval(arr)\n",
    "        seed_dict[lbl] = {'mean': mean, 'ci': [lo, hi]}\n",
    "    # paired t-tests\n",
    "    paired = {}\n",
    "    for a, b in [('BC', 'Random'), ('CQL', 'Random'), ('BC_init', 'Random'), ('CQL', 'BC')]:\n",
    "        t, p = stats.ttest_rel(results[a], results[b])\n",
    "        paired[f'{a}_vs_{b}'] = {'t': t, 'p': p}\n",
    "    seed_dict['paired_tests'] = paired\n",
    "    return seed_dict\n",
    "\n",
    "def main():\n",
    "    # np.random.seed(22)\n",
    "    # torch.manual_seed(22)\n",
    "\n",
    "    # 1. load full dataset\n",
    "    df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "    # 2. create global encoder on full data\n",
    "    categorical_features = ['misconception_type', 'previous_action_id',\n",
    "                           'listen_to_feedback', 'correct_solution',\n",
    "                           'next_action_hint_strength']\n",
    "\n",
    "    global_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    global_encoder.fit(df[categorical_features])\n",
    "\n",
    "    # 3. split into train/test\n",
    "    train_df, val_df, test_df = split_dataframe_by_episodes(df, test_size=0.2, val_size=0.1, random_state=22)\n",
    "\n",
    "    # 4. train BC on TRAINING DATA only\n",
    "    train_states, val_states, train_actions, val_actions, num_actions = prepare_data_for_bc(train_df, encoder=global_encoder)\n",
    "    bc_model = train_bc_model(train_states, val_states, train_actions, val_actions, num_actions)\n",
    "\n",
    "    # define action mappings\n",
    "    # original action IDs to model indices (0-49)\n",
    "    action_map = {0: 0, 1: 1, 2: 2, 3: 3, 5: 4, 6: 5, 7: 6, 8: 7, 11: 8, 12: 9,\n",
    "                 13: 10, 16: 11, 17: 12, 18: 13, 20: 14, 21: 15, 22: 16, 23: 17,\n",
    "                 26: 18, 27: 19, 28: 20, 31: 21, 32: 22, 36: 23, 37: 24, 38: 25,\n",
    "                 41: 26, 42: 27, 43: 28, 45: 29, 46: 30, 47: 31, 48: 32, 54: 33,\n",
    "                 55: 34, 56: 35, 57: 36, 58: 37, 59: 38, 60: 39, 65: 40, 66: 41,\n",
    "                 67: 42, 70: 43, 71: 44, 72: 45, 73: 46, 75: 47, 76: 48, 77: 49}\n",
    "\n",
    "    action_meta_map = {\n",
    "        \"Focus\": {\n",
    "            \"Seek Next Step\": {1: 0, 2: 1, 3: 2},\n",
    "            \"Confirm Calculation\": {1: 5, 2: 6, 3: 7, 4: 8},\n",
    "            \"Re-direct to Sub-Problem\": {2: 11, 3: 12, 4: 13},\n",
    "            \"Highlight Missing Info\": {2: 16, 3: 17, 4: 18}\n",
    "        },\n",
    "        \"Probing\": {\n",
    "            \"Ask for Explanation\": {1: 20, 2: 21, 3: 22, 4: 23},\n",
    "            \"Seek Self-Correction\": {2: 26, 3: 27, 4: 28},\n",
    "            \"Hypothetical Variation\": {2: 31, 3: 32},\n",
    "            \"Check Understanding/Concept\": {2: 36, 3: 37, 4: 38},\n",
    "            \"Encourage Comparison\": {2: 41, 3: 42, 4: 43}\n",
    "        },\n",
    "        \"Telling\": {\n",
    "            \"Partial Reveal (Strategy)\": {1: 45, 2: 46, 3: 47, 4: 48},\n",
    "            \"Full Reveal (Answer)\": {1: 54, 2: 55, 3: 56, 4: 57, 5: 58, 6: 59},\n",
    "            \"Corrective Explanation\": {1: 60}\n",
    "        },\n",
    "        \"Generic\": {\n",
    "            \"Acknowledgment/Praise\": {1: 65, 2: 66, 3: 67},\n",
    "            \"Summarize Progress\": {1: 70, 2: 71, 3: 72, 4: 73},\n",
    "            \"General Inquiry/Filler\": {1: 75, 2: 76, 3: 77}\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # build the transition buffers (train/test) with reward function\n",
    "    train_buffer, _ = build_transition_buffer(\n",
    "        df=train_df,\n",
    "        reward_fn=hybrid_reward,\n",
    "        meta_map=action_meta_map,\n",
    "        orig_to_idx=action_map,\n",
    "        encoder=global_encoder,\n",
    "        categorical_features=categorical_features\n",
    "    )\n",
    "\n",
    "    val_buffer, _ = build_transition_buffer(\n",
    "        df=val_df,\n",
    "        reward_fn=hybrid_reward,\n",
    "        meta_map=action_meta_map,\n",
    "        orig_to_idx=action_map,\n",
    "        encoder=global_encoder,\n",
    "        categorical_features=categorical_features\n",
    "    )\n",
    "\n",
    "    test_buffer, _ = build_transition_buffer(\n",
    "        df=test_df,\n",
    "        reward_fn=hybrid_reward,\n",
    "        meta_map=action_meta_map,\n",
    "        orig_to_idx=action_map,\n",
    "        encoder=global_encoder,\n",
    "        categorical_features=categorical_features\n",
    "    )\n",
    "\n",
    "    # print buffer statistics\n",
    "    print(f\"*Train Buffer Statistics*\")\n",
    "    print(f\"\\tTotal transitions: {len(train_buffer)}\")\n",
    "    print(f\"\\tState dimension: {train_buffer.states.shape[1]}\")\n",
    "    print(f\"\\tNumber of unique actions: {len(set(train_buffer.actions))}\")\n",
    "\n",
    "    # print buffer statistics\n",
    "    print(f\"*Test Buffer Statistics*\")\n",
    "    print(f\"\\tTotal transitions: {len(test_buffer)}\")\n",
    "    print(f\"\\tState dimension: {test_buffer.states.shape[1]}\")\n",
    "    print(f\"\\tNumber of unique actions: {len(set(test_buffer.actions))}\")\n",
    "\n",
    "    # 1: Training CQL from scratch\n",
    "    print(\"\\nTraining CQL from scratch!\")\n",
    "    cql_agent = train_cql(\n",
    "        buffer=train_buffer,\n",
    "        eval_buffer=val_buffer,\n",
    "        state_dim=train_buffer.states.shape[1],\n",
    "        action_dim=num_actions,\n",
    "        num_epochs=100,\n",
    "        batch_size=128,\n",
    "        hidden_dim=256,\n",
    "        lr=1e-4,\n",
    "        gamma=0.98,\n",
    "        tau=0.005,\n",
    "        cql_alpha=2.0,\n",
    "        model_save_path='best_cql_model.pt',\n",
    "        action_map=action_map,\n",
    "        action_meta_map=action_meta_map\n",
    "    )\n",
    "\n",
    "    # 2: Initializing CQL with BC weights\n",
    "    print(\"\\nTraining BC-init CQL!\")\n",
    "    bc_initialized_cql = convert_bc_to_cql_model(bc_model, num_actions)\n",
    "    bc_initialized_cql.save('bc_initialized_cql.pt')\n",
    "\n",
    "    bc_init_cql_agent = train_cql(\n",
    "        buffer=train_buffer,\n",
    "        eval_buffer=val_buffer,\n",
    "        state_dim=train_buffer.states.shape[1],\n",
    "        action_dim=num_actions,\n",
    "        num_epochs=40,\n",
    "        batch_size=128,\n",
    "        hidden_dim=256,\n",
    "        lr=3e-5,\n",
    "        gamma=0.98,\n",
    "        tau=0.005,\n",
    "        cql_alpha=0.85,\n",
    "        model_save_path='best_initialized_cql_model.pt',\n",
    "        action_map=action_map,\n",
    "        action_meta_map=action_meta_map\n",
    "    )\n",
    "\n",
    "    bc_agent = BCPolicyWrapper(bc_model, action_map, device=cql_agent.device)\n",
    "\n",
    "    # make random agent\n",
    "    random_agent = RandomPolicy(num_actions)\n",
    "\n",
    "    agents = [bc_agent, cql_agent, bc_init_cql_agent, random_agent]\n",
    "    labels = ['BC', 'CQL', 'BC_init', 'Random']\n",
    "    # run over 5 seeds\n",
    "    seeds = [0, 1, 2, 3, 4]\n",
    "    all_seeds = [collect_seed_stats(s,\n",
    "                                    agents,\n",
    "                                    labels,\n",
    "                                    buffer=test_buffer,\n",
    "                                    reward_fn=hybrid_reward,\n",
    "                                    action_meta_map=action_meta_map)\n",
    "                 for s in seeds]\n",
    "\n",
    "    np.save('all_seeds.npy', all_seeds)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
