{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "235ed735-40ca-4d1f-a409-1c146204d867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ecd2198-6215-472e-82a5-d79d578cca4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts numpy arrays to Pytorch tensors for custom dataset\n",
    "class TutorDataset(Dataset):\n",
    "    def __init__(self, states, actions):\n",
    "        self.states = torch.tensor(states, dtype=torch.float32)\n",
    "        self.actions = torch.tensor(actions, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.actions)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.states[i], self.actions[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fee75ccd-e11f-4bae-a52c-d43f9055771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stores sequences of state-action-reward transitions (s,a,r,s',a',done)\n",
    "class TransitionBuffer:\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.next_states = []\n",
    "        self.next_actions = []\n",
    "        self.dones = []\n",
    "\n",
    "    def push(self, s, a, r, s2, a2, done):\n",
    "        self.states.append(s)\n",
    "        self.actions.append(a)\n",
    "        self.rewards.append(r)\n",
    "        self.next_states.append(s2)\n",
    "        self.next_actions.append(a2)\n",
    "        \n",
    "        self.dones.append(done)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.actions)\n",
    "\n",
    "# converts tabular tutoring data into transitions, encode features, organize data based on convo turns\n",
    "def build_transition_buffer(\n",
    "    df: pd.DataFrame,\n",
    "    reward_fn,\n",
    "    meta_map: dict,\n",
    "    orig_to_idx: dict,\n",
    "    episode_column: str = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a transition buffer for offline RL using a 50-action index map and\n",
    "    its corresponding metadata map.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with tutoring data; must contain 'next_action_id' and state features.\n",
    "        reward_fn: function(prev_state, action_idx, next_state, action_meta) -> reward\n",
    "        meta_map: nested metadata dict mapping category->strategy->(level->idx) for 50 actions\n",
    "        orig_to_idx: dict mapping original action IDs -> compact indices (0..49)\n",
    "        episode_column: optional col to group episodes; if None assumes ordered by 'convo_turn'\n",
    "\n",
    "    Returns:\n",
    "        buffer: TransitionBuffer of (state, action_idx, reward, next_state, done)\n",
    "        orig_to_idx: same dict mapping original IDs -> indices (for model sizing)\n",
    "    \"\"\"\n",
    "    # Flatten meta_map into flat_idx_to_meta\n",
    "    flat_idx_to_meta = {}\n",
    "    for cat, strategies in meta_map.items():\n",
    "        for strat, levels in strategies.items():\n",
    "            for lvl, idx in levels.items():\n",
    "                flat_idx_to_meta[idx] = {\n",
    "                    'category': cat,\n",
    "                    'strategy': strat,\n",
    "                    'level': lvl\n",
    "                }\n",
    "\n",
    "    # State feature setup\n",
    "    state_feats = ['misconception_type','convo_turn','previous_action_id',\n",
    "                   'listen_to_feedback','problem_progress','progress_delta',\n",
    "                   'correct_solution','next_action_hint_strength']\n",
    "    cat_feats = ['misconception_type','previous_action_id',\n",
    "                 'listen_to_feedback','correct_solution']\n",
    "    num_feats = [f for f in state_feats if f not in cat_feats]\n",
    "\n",
    "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    encoder.fit(df[cat_feats])\n",
    "\n",
    "    # Group episodes\n",
    "    if episode_column:\n",
    "        episodes = df.sort_values([episode_column,'convo_turn']).groupby(episode_column)\n",
    "    else:\n",
    "        df = df.sort_values('convo_turn')\n",
    "        df['_tmp_ep'] = 1\n",
    "        episodes = df.groupby('_tmp_ep')\n",
    "\n",
    "    buffer = TransitionBuffer()\n",
    "\n",
    "    # Build transitions\n",
    "    for _, ep_df in episodes:\n",
    "        prev_state = None\n",
    "        prev_idx = None\n",
    "        prev_meta = None\n",
    "\n",
    "        for _, row in ep_df.iterrows():\n",
    "            orig_id = row.get('next_action_id')\n",
    "            if pd.isna(orig_id) or int(orig_id) not in orig_to_idx:\n",
    "                continue\n",
    "            orig_id = int(orig_id)\n",
    "            idx = orig_to_idx[orig_id]\n",
    "\n",
    "            # encode state\n",
    "            cat = encoder.transform(pd.DataFrame([row[cat_feats]], columns=cat_feats))\n",
    "            num = row[num_feats].to_numpy()\n",
    "            state = np.hstack((num, cat.flatten()))\n",
    "\n",
    "            meta = flat_idx_to_meta.get(idx)\n",
    "            # Read done flag from data\n",
    "            is_done = row.get('done', False)  # Use the actual 'done' column\n",
    "\n",
    "            if prev_state is not None:\n",
    "                if is_done > 0.0:\n",
    "                    terminal_r = new_terminal(prev_state)\n",
    "                    buffer.push(prev_state,\n",
    "                                  prev_idx,\n",
    "                                  terminal_r,\n",
    "                                  state,\n",
    "                                  idx,\n",
    "                                  done=True)\n",
    "            \n",
    "                else:\n",
    "                    r = reward_fn(prev_state, prev_idx, state, prev_meta)\n",
    "                    buffer.push(prev_state, prev_idx, r, state, idx, done=is_done)\n",
    "\n",
    "            prev_state = state\n",
    "            prev_idx = idx\n",
    "            prev_meta = meta\n",
    "    # convert lists to arrays\n",
    "    buffer.states = np.array(buffer.states)\n",
    "    buffer.actions = np.array(buffer.actions, dtype=int)\n",
    "    buffer.rewards = np.array(buffer.rewards, dtype=float)\n",
    "    buffer.next_states = np.array(buffer.next_states)\n",
    "    buffer.next_actions = np.array(buffer.next_actions)\n",
    "    buffer.dones = np.array(buffer.dones, dtype=bool)\n",
    "\n",
    "    print(buffer.states.shape)\n",
    "    return buffer, orig_to_idx\n",
    "\n",
    "\n",
    "# Maximum raw progress\n",
    "MAX_PROGRESS = 50.0\n",
    "def new_terminal(state):\n",
    "    raw_progress = state[4]  # problem_progress in state\n",
    "    normalized_progress = min(raw_progress, MAX_PROGRESS) / MAX_PROGRESS\n",
    "    # Higher reward if correct solution was achieved\n",
    "    return 5.0 if state[6] > 0 else 2.0 * normalized_progress\n",
    "\n",
    "\"\"\"\n",
    "   The big idea behind the reward function: Good tutors don't just give answers right away... they start with questions \n",
    "   and guidance to help students think for themselves, \n",
    "   then provide more direct help if the student struggles. \n",
    "   This \"scaffold then tell\" approach is built into the reward function.\n",
    "\n",
    "   Key design principles:\n",
    "   1. Always reward student progress, regardless of tutor strategy\n",
    "   2. Early in a conversation: reward scaffolding (questions, focus), penalize giving answers\n",
    "   3. Later in a conversation: reduce scaffolding bonuses, reward effective direct instruction\n",
    "   4. Use a smooth transition between these phases rather than an abrupt switch\n",
    "   5. Scale penalties based on how \"telling\" the hint is (revealing answers vs gentle hints)\n",
    "   6. Keep it simple and avoid hard thresholds or complex calculations\n",
    "\n",
    "   The reward transitions happen over ~8 turns, which is carefully calibrated for our dataset:\n",
    "   - With average episode lengths of 15-25 turns, using 8 as the transition denominator makes it so that:\n",
    "     * First ~3 turns: Strong scaffolding emphasis (turn_progress < 0.4)\n",
    "     * Middle ~4 turns: Balanced transition phase (turn_progress 0.4-0.7)\n",
    "     * Remaining turns: Increasing emphasis on problem progress\n",
    "   - This 8-turn transition worked better than:\n",
    "     * 5-turn (too fast, not enough guiding emphasis)\n",
    "     * 10-turn (too slow, many episodes ended before reaching full direct instruction phase)\n",
    "\"\"\"\n",
    "def hybrid_reward(state, action_id, next_state=None, action_meta=None):\n",
    "    # Extract key state information\n",
    "    progress_delta = state[5]\n",
    "    turn = state[1]\n",
    "\n",
    "    # Core progress reward and step penalty\n",
    "    progress_reward = 5.0 * progress_delta\n",
    "    step_penalty = -0.1\n",
    "\n",
    "    # Single transition factor (0 to 1) based on turn number\n",
    "    # Represents how far into the conversation we are\n",
    "    turn_progress = min(1.0, turn / 8.0)\n",
    "\n",
    "    # Action-specific adjustment\n",
    "    strategy_bonus = 0.0\n",
    "\n",
    "    if action_meta is not None:\n",
    "        cat = action_meta['category']\n",
    "\n",
    "        # Guiding/scaffolding actions (Focus, Probing)\n",
    "        if cat in ['Focus', 'Probing']:\n",
    "            # Decreasing bonus for scaffolding actions as conversation progresses\n",
    "            strategy_bonus = 0.2 * (1.0 - turn_progress)\n",
    "\n",
    "        # Telling actions\n",
    "        elif cat == 'Telling':\n",
    "            # Simple hint severity factor\n",
    "            severity = 1.0\n",
    "            if 'strategy' in action_meta:\n",
    "                if action_meta['strategy'] == 'Full Reveal (Answer)':\n",
    "                    severity = 1.5\n",
    "                elif action_meta['strategy'] == 'Conceptual Hint':\n",
    "                    severity = 0.6\n",
    "\n",
    "            # Early: penalty for telling, Late: bonus for effective telling\n",
    "            early_penalty = -0.3 * (1.0 - turn_progress) * severity\n",
    "            late_bonus = 0.0\n",
    "            if progress_delta > 0:  # Only reward effective telling\n",
    "                late_bonus = 0.2 * turn_progress\n",
    "\n",
    "            strategy_bonus = early_penalty + late_bonus\n",
    "\n",
    "    return progress_reward + step_penalty + strategy_bonus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1611404b-1b17-40c7-ac63-b39a82745d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12016, 64)\n"
     ]
    }
   ],
   "source": [
    "# Use the predefined action mappings from system\n",
    "action_map = {0: 0, 1: 1, 2: 2, 3: 3, 5: 4, 6: 5, 7: 6, 8: 7, 11: 8, 12: 9,\n",
    "          13: 10, 16: 11, 17: 12, 18: 13, 20: 14, 21: 15, 22: 16, 23: 17,\n",
    "          26: 18, 27: 19, 28: 20, 31: 21, 32: 22, 36: 23, 37: 24, 38: 25,\n",
    "          41: 26, 42: 27, 43: 28, 45: 29, 46: 30, 47: 31, 48: 32, 54: 33,\n",
    "          55: 34, 56: 35, 57: 36, 58: 37, 59: 38, 60: 39, 65: 40, 66: 41,\n",
    "          67: 42, 70: 43, 71: 44, 72: 45, 73: 46, 75: 47, 76: 48, 77: 49}\n",
    "\n",
    "# Define the action metadata map\n",
    "action_meta_map = {\n",
    "    \"Focus\": {\n",
    "        \"Seek Next Step\": {1: 0, 2: 1, 3: 2},\n",
    "        \"Confirm Calculation\": {1: 5, 2: 6, 3: 7, 4: 8},\n",
    "        \"Re-direct to Sub-Problem\": {2: 11, 3: 12, 4: 13},\n",
    "        \"Highlight Missing Info\": {2: 16, 3: 17, 4: 18}\n",
    "    },\n",
    "    \"Probing\": {\n",
    "        \"Ask for Explanation\": {1: 20, 2: 21, 3: 22, 4: 23},\n",
    "        \"Seek Self-Correction\": {2: 26, 3: 27, 4: 28},\n",
    "        \"Hypothetical Variation\": {2: 31, 3: 32},\n",
    "        \"Check Understanding/Concept\": {2: 36, 3: 37, 4: 38},\n",
    "        \"Encourage Comparison\": {2: 41, 3: 42, 4: 43}\n",
    "    },\n",
    "    \"Telling\": {\n",
    "        \"Partial Reveal (Strategy)\": {1: 45, 2: 46, 3: 47, 4: 48},\n",
    "        \"Full Reveal (Answer)\": {1: 54, 2: 55, 3: 56, 4: 57, 5: 58, 6: 59},\n",
    "        \"Corrective Explanation\": {1: 60}\n",
    "    },\n",
    "    \"Generic\": {\n",
    "        \"Acknowledgment/Praise\": {1: 65, 2: 66, 3: 67},\n",
    "        \"Summarize Progress\": {1: 70, 2: 71, 3: 72, 4: 73},\n",
    "        \"General Inquiry/Filler\": {1: 75, 2: 76, 3: 77}\n",
    "    }\n",
    "}\n",
    "\n",
    "feature_names = [\n",
    "    'misconception_type', 'convo_turn', 'previous_action_id',\n",
    "    'listen_to_feedback', 'problem_progress', 'progress_delta',\n",
    "    'correct_solution', 'next_action_hint_strength'\n",
    "]\n",
    "\n",
    "csv_file_path = \"data.csv\"\n",
    "# First build the transition buffer with all required parameters\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "buffer, action_mapping = build_transition_buffer(\n",
    "    df=df,\n",
    "    reward_fn=hybrid_reward,\n",
    "    meta_map=action_meta_map,\n",
    "    orig_to_idx=action_map,\n",
    "    episode_column=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0595d625-10f3-45d8-a769-7edfc5536b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9612.800000000001"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12016 * 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9616b908-6aa4-4ec0-a95b-d6e5141bf07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_buffer = TransitionBuffer()\n",
    "train_buffer.states = buffer.states[:9612]\n",
    "train_buffer.actions = buffer.actions[:9612]\n",
    "train_buffer.rewards = buffer.rewards[:9612]\n",
    "train_buffer.next_states = buffer.next_states[:9612]\n",
    "train_buffer.next_actions = buffer.next_actions[:9612]\n",
    "train_buffer.dones = buffer.dones[:9612]\n",
    "\n",
    "test_buffer = TransitionBuffer()\n",
    "test_buffer.states = buffer.states[9612:]\n",
    "test_buffer.actions = buffer.actions[9612:]\n",
    "test_buffer.rewards = buffer.rewards[9612:]\n",
    "test_buffer.next_states = buffer.next_states[9612:]\n",
    "test_buffer.next_actions = buffer.next_actions[9612:]\n",
    "test_buffer.dones = buffer.dones[9612:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f483a537-b870-4875-829c-276cddce2968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = open('train_replay_buffer.pkl', 'wb')\n",
    "pickle.dump(train_buffer, file)\n",
    "file.close()\n",
    "\n",
    "file = open('test_replay_buffer.pkl', 'wb')\n",
    "pickle.dump(test_buffer, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810a28eb-3cd0-4ab0-9f42-0287de6fdb9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
