{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Imitative Learning"
      ],
      "metadata": {
        "id": "atW4rUfrujdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data loading\n",
        "import pandas as pd\n",
        "dataset = pd.read_csv('FINAL_MATHDIAL - final.csv')"
      ],
      "metadata": {
        "id": "71NOi8ziujO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# divide state and action\n",
        "dataset = dataset.dropna(subset=['next_action_id'])\n",
        "state = dataset[['misconception_type', 'convo_turn','done','listen_to_feedback','problem_progress','progress_delta','correct_solution','next_action_hint_strength']]\n",
        "action = dataset[['next_action_id']]"
      ],
      "metadata": {
        "id": "qxAHcnYLVcsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_cols = ['misconception_type', 'done', 'listen_to_feedback', 'correct_solution', 'next_action_hint_strength']\n",
        "continuous_cols = ['convo_turn', 'problem_progress', 'progress_delta']"
      ],
      "metadata": {
        "id": "Ab_Yn7FLkYlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "state_cat = pd.get_dummies(state[categorical_cols], drop_first=False).reset_index(drop=True)\n",
        "state_cont = pd.DataFrame(\n",
        "    scaler.fit_transform(state[continuous_cols]),\n",
        "    columns=continuous_cols\n",
        ").reset_index(drop=True)\n",
        "\n",
        "state_processed = pd.concat([state_cat, state_cont], axis=1)\n",
        "state_tensor = torch.tensor(state_processed.values, dtype=torch.float32)\n",
        "action_tensor = torch.tensor(action.values, dtype=torch.long)"
      ],
      "metadata": {
        "id": "_YZvXO7nX79V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "class ImitationDataset(Dataset):\n",
        "    def __init__(self, states, actions):\n",
        "        self.states = states\n",
        "        self.actions = actions\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.states)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.states[idx], self.actions[idx]\n",
        "\n",
        "dataset = ImitationDataset(state_tensor, action_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "jkeh4c8AXUkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(state_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, action_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        init.kaiming_uniform_(self.fc1.weight, nonlinearity='relu')\n",
        "        init.zeros_(self.fc1.bias)\n",
        "        init.kaiming_uniform_(self.fc2.weight, nonlinearity='relu')\n",
        "        init.zeros_(self.fc2.bias)\n",
        "        init.xavier_uniform_(self.fc3.weight)\n",
        "        init.zeros_(self.fc3.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "ULQ6cZgPWvT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "input_dim = state_tensor.shape[1]\n",
        "output_dim = action_tensor.max().item() + 1\n",
        "\n",
        "model = PolicyNetwork(input_dim, output_dim)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for batch_states, batch_actions in dataloader:\n",
        "        batch_actions -= batch_actions.min()\n",
        "\n",
        "        outputs = model(batch_states)\n",
        "\n",
        "        loss = criterion(outputs, batch_actions.squeeze())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Total Loss: {total_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGBRW_-mW_V4",
        "outputId": "74a829b0-9f4e-4695-87df-43615b5c0747"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Total Loss: 503.3179\n",
            "Epoch 2/100, Total Loss: 454.2665\n",
            "Epoch 3/100, Total Loss: 439.9753\n",
            "Epoch 4/100, Total Loss: 419.4604\n",
            "Epoch 5/100, Total Loss: 422.8289\n",
            "Epoch 6/100, Total Loss: 422.4335\n",
            "Epoch 7/100, Total Loss: 407.5324\n",
            "Epoch 8/100, Total Loss: 413.5846\n",
            "Epoch 9/100, Total Loss: 416.0070\n",
            "Epoch 10/100, Total Loss: 413.1643\n",
            "Epoch 11/100, Total Loss: 403.7424\n",
            "Epoch 12/100, Total Loss: 414.7692\n",
            "Epoch 13/100, Total Loss: 401.1116\n",
            "Epoch 14/100, Total Loss: 411.7721\n",
            "Epoch 15/100, Total Loss: 407.0780\n",
            "Epoch 16/100, Total Loss: 399.3377\n",
            "Epoch 17/100, Total Loss: 397.3970\n",
            "Epoch 18/100, Total Loss: 394.2145\n",
            "Epoch 19/100, Total Loss: 401.7543\n",
            "Epoch 20/100, Total Loss: 407.2274\n",
            "Epoch 21/100, Total Loss: 398.0978\n",
            "Epoch 22/100, Total Loss: 418.2607\n",
            "Epoch 23/100, Total Loss: 404.8504\n",
            "Epoch 24/100, Total Loss: 412.8212\n",
            "Epoch 25/100, Total Loss: 399.0020\n",
            "Epoch 26/100, Total Loss: 404.3587\n",
            "Epoch 27/100, Total Loss: 402.4156\n",
            "Epoch 28/100, Total Loss: 402.6145\n",
            "Epoch 29/100, Total Loss: 399.6480\n",
            "Epoch 30/100, Total Loss: 404.0747\n",
            "Epoch 31/100, Total Loss: 398.5867\n",
            "Epoch 32/100, Total Loss: 399.9113\n",
            "Epoch 33/100, Total Loss: 404.2279\n",
            "Epoch 34/100, Total Loss: 393.6988\n",
            "Epoch 35/100, Total Loss: 407.4118\n",
            "Epoch 36/100, Total Loss: 396.2515\n",
            "Epoch 37/100, Total Loss: 401.7928\n",
            "Epoch 38/100, Total Loss: 403.6013\n",
            "Epoch 39/100, Total Loss: 399.3362\n",
            "Epoch 40/100, Total Loss: 398.4430\n",
            "Epoch 41/100, Total Loss: 377.1063\n",
            "Epoch 42/100, Total Loss: 406.9743\n",
            "Epoch 43/100, Total Loss: 396.1584\n",
            "Epoch 44/100, Total Loss: 402.9395\n",
            "Epoch 45/100, Total Loss: 404.7076\n",
            "Epoch 46/100, Total Loss: 395.4654\n",
            "Epoch 47/100, Total Loss: 397.0914\n",
            "Epoch 48/100, Total Loss: 400.5963\n",
            "Epoch 49/100, Total Loss: 396.7735\n",
            "Epoch 50/100, Total Loss: 391.0949\n",
            "Epoch 51/100, Total Loss: 398.7604\n",
            "Epoch 52/100, Total Loss: 389.4769\n",
            "Epoch 53/100, Total Loss: 395.0847\n",
            "Epoch 54/100, Total Loss: 405.8244\n",
            "Epoch 55/100, Total Loss: 393.4824\n",
            "Epoch 56/100, Total Loss: 400.3874\n",
            "Epoch 57/100, Total Loss: 395.9465\n",
            "Epoch 58/100, Total Loss: 389.1222\n",
            "Epoch 59/100, Total Loss: 402.7054\n",
            "Epoch 60/100, Total Loss: 391.9429\n",
            "Epoch 61/100, Total Loss: 404.7384\n",
            "Epoch 62/100, Total Loss: 401.5392\n",
            "Epoch 63/100, Total Loss: 386.0470\n",
            "Epoch 64/100, Total Loss: 405.0924\n",
            "Epoch 65/100, Total Loss: 402.5728\n",
            "Epoch 66/100, Total Loss: 400.9278\n",
            "Epoch 67/100, Total Loss: 399.5187\n",
            "Epoch 68/100, Total Loss: 397.6428\n",
            "Epoch 69/100, Total Loss: 406.8226\n",
            "Epoch 70/100, Total Loss: 400.6017\n",
            "Epoch 71/100, Total Loss: 410.3624\n",
            "Epoch 72/100, Total Loss: 397.1832\n",
            "Epoch 73/100, Total Loss: 402.1492\n",
            "Epoch 74/100, Total Loss: 396.8479\n",
            "Epoch 75/100, Total Loss: 406.7663\n",
            "Epoch 76/100, Total Loss: 394.0881\n",
            "Epoch 77/100, Total Loss: 391.6816\n",
            "Epoch 78/100, Total Loss: 394.2549\n",
            "Epoch 79/100, Total Loss: 401.5067\n",
            "Epoch 80/100, Total Loss: 397.2751\n",
            "Epoch 81/100, Total Loss: 399.1031\n",
            "Epoch 82/100, Total Loss: 397.5084\n",
            "Epoch 83/100, Total Loss: 405.4632\n",
            "Epoch 84/100, Total Loss: 384.2629\n",
            "Epoch 85/100, Total Loss: 403.3332\n",
            "Epoch 86/100, Total Loss: 392.4354\n",
            "Epoch 87/100, Total Loss: 386.6206\n",
            "Epoch 88/100, Total Loss: 391.9643\n",
            "Epoch 89/100, Total Loss: 395.5839\n",
            "Epoch 90/100, Total Loss: 395.3756\n",
            "Epoch 91/100, Total Loss: 388.1218\n",
            "Epoch 92/100, Total Loss: 393.4115\n",
            "Epoch 93/100, Total Loss: 399.6430\n",
            "Epoch 94/100, Total Loss: 403.9053\n",
            "Epoch 95/100, Total Loss: 399.0948\n",
            "Epoch 96/100, Total Loss: 404.7680\n",
            "Epoch 97/100, Total Loss: 401.3900\n",
            "Epoch 98/100, Total Loss: 403.2552\n",
            "Epoch 99/100, Total Loss: 389.3612\n",
            "Epoch 100/100, Total Loss: 383.9560\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}